---
title: "Prediction_project"
author: "Albert Palleja"
date: "06/07/2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#Sys.putenv("DISPLAY"=":0.0")

## IMPORTANT NOTE: I apologize upfront because I could not generate an HTML file. I work directly with RStudio configured to work in a server. I just found out that I am missing the pango cairo library, which makes impossible to generate reports using knit. See the following information:
#https://support.rstudio.com/hc/en-us/community/posts/200642948-RStudio-Server-unable-to-open-connection-to-X11-display

#So I hope you can grade me by checking my Rmarkdown document. Thanks in advance!
```

## R Markdown document

This is an R Markdown document dedicated to the final prediction project for the Practical Machine learning course at Coursera.

```{r reading data}
# Already working in the directory I downloaded the data from 
#https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
#https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
# Loading the training and testing data and checking it
training = read.csv("pml-training.csv", header=TRUE, na.strings = c("NA", "", "#DIV/0!"))
testing = read.csv("pml-testing.csv", header=TRUE, na.strings = c("NA", "", "#DIV/0!"))
#head(training)
dim(training)
#head(testing)
dim(testing)
```

# Cleaning up the data
``` {r }
library(caret)

# I remove variables containing NA
no.NA<-apply(training, 2, function(x) !any(is.na(x)))

#no.NA

training.noNA<-training[, no.NA]

dim(training.noNA) # I end up with 60 predictors

# I remove some predictors that seem to me irrelevant for the prediction:
training.noNA.clean<-training.noNA[,-c(1:7)]
dim(training.noNA.clean)  # I end up with 53 predictors

# I finally remove predictors with near zero variance
predictors.near0Var<-nearZeroVar(training.noNA.clean, saveMetrics = TRUE)
head(predictors.near0Var)
train.clean<-training.noNA.clean[,predictors.near0Var$nzv==FALSE]
dim(train.clean) # At the end I am left with 52 predictors plus the variable to predict (classe)

# I apply the same preprocessing to the testing set (validation set):
test.clean<-testing[, names(train.clean[,-53])]
dim(test.clean)
```

# Partitioning the data into training and testing data
``` {r partitioning data}

# I split the training data into a train set and a test set. The latter one will be used to estimate the out of sample error
# Considering we have a rather large sample size, the data is partitioned into 60% samples for training and 40% samples for testing. The testing set provided I will keep it for validation
set.seed(333)
inTrain = createDataPartition(train.clean$classe, p = 0.6, list=FALSE)

training = train.clean[ inTrain,]
head(training)
dim(training) # 11,776 samples for fitting a model

testing = train.clean[-inTrain,]
head(testing)
dim(testing) # 7,846 samples to predict on and find out the out of samples error

```

# Model fitting and prediction
``` {r prediction}
# I use a bunch of machine learning advanced classification algorithms to assess which of them provides the lowest out of error sample for this project

## Prediction with decision trees
# Needed to grow a tree
library(rpart)
# To draw a pretty tree
#source("https://bioconductor.org/biocLite.R")
#biocLite("rattle")
#biocLite("GTK")
#library(rattle)

set.seed(12345)
modFit.tree<-rpart(classe~., method="class", data = training)
#modFit.tree<-rpart(classe~., data = training, method = "class")
# I could not install rattle because an R version problem for a package that rattle depends on (GTK and )
# I print and plot the final model variable to find out which predictors drive the splits in the decision tree
print(modFit.tree)
plot(modFit.tree, uniform=TRUE, main="Classification tree")
text(modFit.tree, use.n=TRUE, all=TRUE, cex=.8)

pred.train.tree<-predict(modFit.tree, newdata = training, type = "class")
confusionMatrix(pred.train.tree, training$classe)

pred.tree<-predict(modFit.tree, newdata=testing,  type = "class")
confusionMatrix(pred.tree, testing$classe)

#Accuracy for in-sample is 0.7475 and for out-sample is 0.7388. SO the algorithm performs quite well predicting in the training and testing set. As expected the out-of sample error is higher that the in-sample error.

## Predicting by random forest
# Now we are growing different trees introducing some randomness. This method is more powerful and usually yields better accuracy than only using one tree as I did above.
# To avoid overfitting we use cross validation (10-fold)
fitControl<-trainControl(method="cv", number=10, verbose=FALSE)
modFit.rf<-train(classe~., data=training, method="rf", trControl=fitControl, verbose=FALSE)
print(modFit.rf)

pred.train.rf<-predict(modFit.rf, newdata = training)
confusionMatrix(pred.train.rf, training$classe)

pred.rf<-predict(modFit.rf, newdata = testing)
confusionMatrix(pred.rf, testing$classe)

#Accuracy for in-sample is 1 and for out-sample is 0.9915. So the algorithm performs amazingly well predicting if the exercise has been done in the right way (classA) or in a wrong way (the other classes). 
# Predicting with generalized boosted regression
# To assess if this algorithm can do better than random forest as it usually performs with high accuracy
modFit.gbm<-train(classe~., data=training, method="gbm", trControl=fitControl, verbose=FALSE)
print(modFit.gbm$finalModel)

pred.train.gbm<-predict(modFit.gbm, newdata=training)
confusionMatrix(pred.train.gbm, training$classe)

pred.gbm<-predict(modFit.gbm, newdata = testing)
confusionMatrix(pred.gbm, testing$classe)

#Accuracy for in-sample is 0.9734 and for out-sample is 0.9579. So this algorithm performs also pretty well predicting the different ways to do the exercise.

```

# Prediction in the validation test
``` {r validation}
# Since the we obtain the highest accuracy with random forest (0.9915) we proceed with that algorithm to predict within the validation test
pred.valid.test<-predict(modFit.rf, newdata = test.clean)
print(pred.valid.test)

# [1] B A B A A E D B A A B C B A E E A B B B
#Levels: A B C D E
```